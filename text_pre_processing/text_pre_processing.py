import re
import string
from underthesea import word_tokenize

class TextPreProcessing(object):
    def __init__(self):
        self.punctuation = '!"#$%&\'()*+,-/:;<=>?@[\\]^_`{|}~'

    def normalize_money(self, sent):
        return re.sub(r'[0-9]+[.,0-9][k-m-b]', 'giÃ¡', sent)

    def normalize_hastag(self, sent):
        return re.sub(r'#+\w+', 'tag', sent)

    def normalize_website(self, sent):
        result = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'website', sent)
        return re.sub(r'\w+(\.(com|vn|me))+((\/+([\.\w\_\-]+)?)+)?', 'website', result)

    def nomalize_emoji(self, sent):
        emoji_pattern = re.compile("["
                    u"\U0001F600-\U0001F64F"  # emoticons
                    u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                    u"\U0001F680-\U0001F6FF"  # transport & map symbols
                    u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                    u"\U00002702-\U000027B0"
                    u"\U000024C2-\U0001F251"
                    u"\U0001f926-\U0001f937"
                    u'\U00010000-\U0010ffff'
                    u"\u200d"
                    u"\u2640-\u2642"
                    u"\u2600-\u2B55"
                    u"\u23cf"
                    u"\u23e9"
                    u"\u231a"
                    u"\u3030"
                    u"\ufe0f"
        "]+", flags=re.UNICODE)
        return emoji_pattern.sub(r'', sent)

    def normalize_elongate(self, sent):
        patern = r'(.)\1{1,}'
        result = sent
        while(re.search(patern, result) != None):
            repeat_char = re.search(patern, result)
            result = result.replace(repeat_char[0], repeat_char[1])
        return result

    def remove_number(self, sent):
        return re.sub(r'[0-9]+', '', sent)

    def normalize_acronyms(self, sent):
        text = sent
        replace_list = {
            'Ã´ kÃªi': ' ok ', 'okie': ' ok ', ' o kÃª ': ' ok ',
            'okey': ' ok ', 'Ã´kÃª': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',' okay':' ok ','okÃª':' ok ',
            ' tks ': u' cÃ¡m Æ¡n ', 'thks': u' cÃ¡m Æ¡n ', 'thanks': u' cÃ¡m Æ¡n ', 'ths': u' cÃ¡m Æ¡n ', 'thank': u' cÃ¡m Æ¡n ',
            'â­': 'star ', '*': 'star ', 'ðŸŒŸ': 'star ', 'ðŸŽ‰': u' tÃ­ch cá»±c ',
            'kg ': u' khÃ´ng ','not': u' khÃ´ng ', u' kg ': u' khÃ´ng ', '"k ': u' khÃ´ng ',' kh ':u' khÃ´ng ','kÃ´':u' khÃ´ng ','hok':u' khÃ´ng ',' kp ': u' khÃ´ng pháº£i ',u' kÃ´ ': u' khÃ´ng ', '"ko ': u' khÃ´ng ', u' ko ': u' khÃ´ng ', u' k ': u' khÃ´ng ', 'khong': u' khÃ´ng ', u' hok ': u' khÃ´ng ',
            'he he': ' tÃ­ch cá»±c ','hehe': ' tÃ­ch cá»±c ','hihi': ' tÃ­ch cá»±c ', 'haha': ' tÃ­ch cá»±c ', 'hjhj': ' tÃ­ch cá»±c ',
            ' lol ': ' tiÃªu cá»±c ',' cc ': ' tiÃªu cá»±c ','cute': u' dá»… thÆ°Æ¡ng ','huhu': ' tiÃªu cá»±c ', ' vs ': u' vá»›i ', 'wa': ' quÃ¡ ', 'wÃ¡': u' quÃ¡', 'j': u' gÃ¬ ', 'â€œ': ' ',
            ' sz ': u' cá»¡ ', 'size': u' cá»¡ ', u' Ä‘x ': u' Ä‘Æ°á»£c ', 'dk': u' Ä‘Æ°á»£c ', 'dc': u' Ä‘Æ°á»£c ', 'Ä‘k': u' Ä‘Æ°á»£c ',
            'Ä‘c': u' Ä‘Æ°á»£c ','authentic': u' chuáº©n chÃ­nh hÃ£ng ',u' aut ': u' chuáº©n chÃ­nh hÃ£ng ', u' auth ': u' chuáº©n chÃ­nh hÃ£ng ', 'thick': u' tÃ­ch cá»±c ', 'store': u' cá»­a hÃ ng ',
            'shop': u' cá»­a hÃ ng ', 'sp': u' sáº£n pháº©m ', 'gud': u' tá»‘t ','god': u' tá»‘t ','wel done':' tá»‘t ', 'good': u' tá»‘t ', 'gÃºt': u' tá»‘t ',
            'sáº¥u': u' xáº¥u ','gut': u' tá»‘t ', u' tot ': u' tá»‘t ', u' nice ': u' tá»‘t ', 'perfect': 'ráº¥t tá»‘t', 'bt': u' bÃ¬nh thÆ°á»ng ',
            'time': u' thá»i gian ', 'qÃ¡': u' quÃ¡ ', u' ship ': u' giao hÃ ng ', u' m ': u' mÃ¬nh ', u' mik ': u' mÃ¬nh ',
            'ÃªÌ‰': 'á»ƒ', 'product': 'sáº£n pháº©m', 'quality': 'cháº¥t lÆ°á»£ng','chat':' cháº¥t ', 'excelent': 'hoÃ n háº£o', 'bad': 'tá»‡','fresh': ' tÆ°Æ¡i ','sad': ' tá»‡ ',
            'date': u' háº¡n sá»­ dá»¥ng ', 'hsd': u' háº¡n sá»­ dá»¥ng ','quickly': u' nhanh ', 'quick': u' nhanh ','fast': u' nhanh ','delivery': u' giao hÃ ng ',u' sÃ­p ': u' giao hÃ ng ',
            'beautiful': u' Ä‘áº¹p tuyá»‡t vá»i ', u' tl ': u' tráº£ lá»i ', u' r ': u' rá»“i ', u' shopE ': u' cá»­a hÃ ng ',u' order ': u' Ä‘áº·t hÃ ng ',
            'cháº¥t lg': u' cháº¥t lÆ°á»£ng ',u' sd ': u' sá»­ dá»¥ng ',u' dt ': u' Ä‘iá»‡n thoáº¡i ',u' nt ': u' nháº¯n tin ',u' tl ': u' tráº£ lá»i ',u' sÃ i ': u' xÃ i ',u'bjo':u' bao giá» ',
            'thik': u' thÃ­ch ',u' sop ': u' cá»­a hÃ ng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' ráº¥t ',u'quáº£ ng ':u' quáº£ng  ',
            'dep': u' Ä‘áº¹p ',u' xau ': u' xáº¥u ','delicious': u' ngon ', u'hÃ g': u' hÃ ng ', u'qá»§a': u' quáº£ ',
            'iu': u' yÃªu ','fake': u' giáº£ máº¡o ', 'trl': 'tráº£ lá»i', '><': u' tÃ­ch cá»±c ',
            ' por ': u' tá»‡ ',' poor ': u' tá»‡ ', 'ib':u' nháº¯n tin ', 'rep':u' tráº£ lá»i ',u'fback':' feedback ','fedback':' feedback '
        }
        for k, v in replace_list.items():
            text = text.replace(k, v)
        return text

    def normalize(self, sent):
        result = self.normalize_money(sent)
        result = self.normalize_hastag(result)
        result = self.normalize_website(result)
        result = self.nomalize_emoji(result)
        result = self.normalize_elongate(result)
        result = self.normalize_acronyms(result)
        result = self.remove_number(result)
        result = re.sub(r'\.+', ' . ',result)
        return result.translate(str.maketrans(self.punctuation, ' ' * len(self.punctuation))).replace(' '*4, ' ').replace(' '*3, ' ').replace(' '*2, ' ').strip()

    def tokenize(self, sent):
        return re.sub(r'\._+',' . ',word_tokenize(sent, format="text"))

    def start_pre_processing(self, dataframes):
        for df in dataframes:
            df['Review'] = df['Review'].apply(self.normalize).apply(self.tokenize)
        return dataframes